{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Three.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd"
      },
      "source": [
        "# Write your code here\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import requests\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "\n",
        "import collections\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "opinion=[]\n",
        "resource=[\"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"]\n",
        "text=[]\n",
        "head=[]\n",
        "\n",
        "for val in resource:\n",
        "  document=requests.get(val,headers={'User-Agent':'Chrome/85.0.4183.121'})\n",
        "  s = BeautifulSoup(document.content, 'html.parser')\n",
        "  putInArray(s.find_all('div', class_='text show-more__control'),text)\n",
        "  putInArray(s.find_all('a',class_='title'),head)\n",
        "  putInArray(s.find_all(name='span',class_='rating-other-user-rating'),opinion)\n",
        "dfIMDB=pd.DataFrame({'comment':text,'title':head})\n",
        "dfIMDB.to_csv('review.csv')\n",
        "\n",
        "def getFrequencyOfNgram(val,arg):\n",
        "  arr=[]\n",
        "  for i in val:\n",
        "    for j in  NgramCreate(i, arg):\n",
        "      arr.append(j)\n",
        "  return collections.Counter(arr)\n",
        "gThree=getFrequencyOfNgram(dfIMDB['comment'],3)\n",
        "oF=getFrequencyOfNgram(dfIMDB['comment'],1)\n",
        "tF=getFrequencyOfNgram(dfIMDB['comment'],2)\n",
        "\n",
        "\n",
        "def putInArray(values,reference):\n",
        "  for val in values:\n",
        "    reference.append(val.text)\n",
        "    phraseForNoun=[]\n",
        "def  NgramCreate(arg1, arg):\n",
        "  arg1 = arg1.lower()\n",
        "  arg1 = re.sub(r'[^a-zA-Z0-9\\s]', ' ', arg1)\n",
        "  tokens = [token for token in arg1.split(\" \") if token != \"\"]\n",
        "  output = list(ngrams(tokens, arg))\n",
        "  return output\n",
        "\n",
        "\n",
        "nLP = spacy.load(\"en_core_web_sm\")\n",
        "for val in dfIMDB['comment']:\n",
        "  dox = nLP(val)\n",
        "  for chunk in dox.noun_chunks:\n",
        "    phraseForNoun.append(chunk)\n",
        "nounFreq=collections.Counter(phraseForNoun)\n",
        "\n",
        "phraseForNoun[1:15]\n",
        "\n",
        "high=max(nounFreq.values())\n",
        "relF=[]\n",
        "for b in nounFreq.values():\n",
        "  v=b/high\n",
        "  relF.append(v)\n",
        "d={'noun_phrases':phraseForNoun,'relF':relF}\n",
        "relF_df=pd.DataFrame(data=d)\n",
        "relF_df\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "source": [
        "# Write your code here\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from nltk import word_tokenize          \n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "\n",
        "def method(arg, IDs=None):\n",
        "    vect = CountVectorizer(lowercase=True, stop_words=None)\n",
        "    method = vect.fit_transform(arg)\n",
        "    method_feature_names = vect.get_feature_names()\n",
        "    #\n",
        "    dataFrame = pd.DataFrame(method.toarray(), columns=method_feature_names, dtype=\"float64\")\n",
        "    if IDs is not None:\n",
        "        dataFrame.index = IDs    \n",
        "    return dataFrame\n",
        "method(dfIMDB['comment'])\n",
        "s = set(stopwords.words('english')) \n",
        "class TokenLemma:\n",
        "    ig = [':', '\"', '``', \"''\", '`',',','.',';']\n",
        "    def _init_(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def _call_(self, doc):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ig]\n",
        "ft = 'movie'\n",
        "tokenizer=TokenLemma()\n",
        "dc = dfIMDB['comment']\n",
        "ts = tokenizer(' '.join(s))\n",
        "\n",
        "v = TfidfVectorizer(stop_words=ts, \n",
        "                              tokenizer=tokenizer)\n",
        "dv = v.fit_transform([ft] + dc)\n",
        "c = linear_kernel(dv[0:1], dv).flatten()\n",
        "d = [item.item() for item in c[1:]]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "# Link: https://github.com/srenisureneni/INFO-5731/blob/master/sentiment_analysis.csv"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}